import pandas as pdimport pickleimport warningswarnings.filterwarnings('ignore')import refrom sentence_transformers import SentenceTransformer, utilfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArgumentsfrom torch.utils.data import Dataset, DataLoaderfrom sklearn.model_selection import train_test_splitimport pytorch_lightning as plfrom transformers import (AdamW)import torchdf = pd.read_excel("/Users/karthiksagar/DestinX/Technical_interview.xlsx")df_copy = df.copy()df_copy['Answer1'] = df_copy['Answer1'].apply(lambda x: re.sub(r'[^a-zA-Z0-9]', ' ', x))df_copy['Answer1'] = df_copy['Answer1'].apply(lambda x: x.lower())df_copy['Answer2'] = df_copy['Answer2'].apply(lambda x: re.sub(r'[^a-zA-Z0-9]', ' ', x))df_copy['Answer2'] = df_copy['Answer2'].apply(lambda x: x.lower())df_copy['Answer3'] = df_copy['Answer3'].apply(lambda x: re.sub(r'[^a-zA-Z0-9]', ' ', x))df_copy['Answer3'] = df_copy['Answer3'].apply(lambda x: x.lower())df_copy['Answer4'] = df_copy['Answer4'].apply(lambda x: re.sub(r'[^a-zA-Z0-9]', ' ', x))df_copy['Answer4'] = df_copy['Answer4'].apply(lambda x: x.lower())df_copy['Answer5'] = df_copy['Answer5'].apply(lambda x: re.sub(r'[^a-zA-Z0-9]', ' ', x))df_copy['Answer5'] = df_copy['Answer5'].apply(lambda x: x.lower())df_copy['Answer6'] = df_copy['Answer6'].str.replace('[^a-zA-Z0-9]', ' ')df_copy['Answer6'] = df_copy['Answer6'].str.lower()df_copy['user_answer'] = df_copy['user_answer'].apply(lambda x: re.sub(r'[^a-zA-Z0-9]', ' ', x))df_copy['user_answer'] = df_copy['user_answer'].apply(lambda x: x.lower())corpus_ans1 = list(df_copy['Answer1'].values)corpus_ans2 = list(df_copy['Answer2'].values)corpus_ans3 = list(df_copy['Answer3'].values)corpus_ans4 = list(df_copy['Answer4'].values)corpus_ans5 = list(df_copy['Answer5'].values)corpus_ans6 = list(df_copy['Answer6'].values)corpus_user = list(df_copy['user_answer'].values)corpus_questions = list(df_copy['Questions'].values)tokenizer = T5Tokenizer.from_pretrained("t5-base")class CustomDataset(Dataset):    def __init__(self, data: pd.DataFrame, tokenizer: T5Tokenizer, source_max_token_len: int = 396, target_max_token_len: int = 396):      self.tokenizer = tokenizer      self.data = data      self.source_max_token_len = source_max_token_len      self.target_max_token_len = target_max_token_len    def __len__(self):        return len(self.data)    def __getitem__(self, index:int):      data_row = self.data.iloc[index]      source_encoding = tokenizer(        data_row['Questions'],        data_row['CONTEXT'],        max_length = self.source_max_token_len,        padding = "max_length",        truncation = "only_second",        return_attention_mask = True,        add_special_tokens = True,        return_tensors = "pt"      )      target_encoding = tokenizer(        data_row['Answer1'],        data_row['Answer2'],        data_row['Answer3'],        max_length = self.target_max_token_len,        padding = 'max_length',        truncation = True,        return_attention_mask = True,        add_special_tokens = True,        return_tensors = "pt"      )      labels = target_encoding['input_ids']      labels[labels==0] = -100      return dict(          question = data_row['Questions'],          context = data_row['CONTEXT'],          answer1 = data_row['Answer1'],          answer2 = data_row['Answer2'],          answer3 = data_row['Answer3'],          input_ids = source_encoding['input_ids'].flatten(),          attention_mask = source_encoding['attention_mask'].flatten(),          labels = labels.flatten()      )train_df, test_df = train_test_split(df, test_size = 0.3)class DataModule(pl.LightningDataModule):  def __init__(self, train_df: pd.DataFrame, test_df: pd.DataFrame, tokenizer: T5Tokenizer, batch_size = 4, source_max_token_len: int = 396, target_max_token_len: int = 396):    super().__init__()    self.batch_size = batch_size    self.train_df = train_df    self.test_df = test_df    self.tokenizer = tokenizer    self.source_max_token_len = source_max_token_len    self.target_max_token_len = target_max_token_len  def setup(self, stage = None):    self.train_dataset = CustomDataset(self.train_df, self.tokenizer, self.source_max_token_len, self.target_max_token_len)    self.test_dataset = CustomDataset(self.test_df, self.tokenizer, self.source_max_token_len, self.target_max_token_len)  def train_dataloader(self):    return DataLoader(        self.train_dataset,        batch_size = self.batch_size,        shuffle = True,        num_workers = 2,    )  def val_dataloader(self):    return DataLoader(        self.test_dataset,        batch_size = 1,        num_workers = 2,    )  def test_dataloader(self):    return DataLoader(        self.test_dataset,        batch_size = 1,        num_workers = 2,    )  def predict_dataloader(self):    return self.test_dataloader()BATCH_SIZE = 6N_EPOCHS = 6data_module = DataModule(train_df, test_df, tokenizer, batch_size = BATCH_SIZE)data_module.setup()MODEL_NAME = 't5-base'model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict = True)class CustomModel(pl.LightningModule):  def __init__(self):    super().__init__()    self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict = True)  def forward(self, input_ids, attention_mask, labels = None, decoder_input_ids=None):    output = self.model(      input_ids = input_ids,      attention_mask = attention_mask,      labels = labels,      decoder_input_ids = decoder_input_ids    )    return output.loss, output.logits  def training_step(self, batch, batch_idx):    input_ids = batch['input_ids']    attention_mask = batch['attention_mask']    labels = batch['labels']    loss, outputs = self(input_ids, attention_mask, labels)    self.log("train_loss", loss, prog_bar=True, logger=True)    return loss  def validation_step(self, batch, batch_idx):    input_ids = batch['input_ids']    attention_mask = batch['attention_mask']    labels = batch['labels']    loss, outputs = self(input_ids, attention_mask, labels)    self.log("val_loss", loss, prog_bar=True, logger=True)    return loss  def test_step(self, batch, batch_idx):    input_ids = batch['input_ids']    attention_mask = batch['attention_mask']    labels = batch['labels']    loss, outputs = self(input_ids, attention_mask, labels)    self.log("test_loss", loss, prog_bar=True, logger=True)    return loss  def configure_optimizers(self):    return AdamW(self.parameters(), lr = 0.0001)with open('/Users/karthiksagar/DestinX/model.pkl', 'rb') as f:    loaded_model = pickle.load(f)def generate_answer(question):  source_encoding = tokenizer(      question['Questions'],      question['CONTEXT'],      max_length = 396,      padding = 'max_length',      truncation = 'only_second',      return_attention_mask = True,      add_special_tokens = True,      return_tensors = 'pt'  )  pred_id = loaded_model.model.generate(      input_ids = source_encoding['input_ids'],      attention_mask = source_encoding['attention_mask'],      num_beams = 1,      max_length = 200,      repetition_penalty = 2.5,      length_penalty = 1.0,      early_stopping = True  )  pred =[tokenizer.decode(i, skip_special_tokens=True, clean_up_tokenization_spaces=True) for i in pred_id]  return (pred[0])def calculate_similarity(user_embedding, correct_answer_embedding):    return util.pytorch_cos_sim(user_embedding, correct_answer_embedding)[0][0].item()def check_similarity(user_answer, correct_answers, index, similarity_threshold=0.98):    # Load the model    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')    # Ensure user_answer is a string    user_answer = str(user_answer)    # Encode the user and correct answers into sentence embeddings    user_embedding = model.encode(user_answer, convert_to_tensor=True)    correct_answers_embedding = [model.encode(str(answer), convert_to_tensor=True) for answer in correct_answers]    # Calculate cosine similarity between the user and each correct answer    similarity_scores = [calculate_similarity(user_embedding, ans_embedding) for ans_embedding in correct_answers_embedding]    max_similarity_score = max(similarity_scores)    # Check if the list is not empty before using max    genanswer = ""    finalist=[]    if max_similarity_score > similarity_threshold:        # Check if the maximum similarity score is above the threshold        #print(max_similarity_score)        finalist.append(max_similarity_score)        genanswer= "Your answer is good enough...congrats!"        finalist.append(genanswer)        return finalist    else:        # If the list is empty, all scores are below the threshold        #print("well tried, instead you can answer to this question in this way, \n")        sample_question = df.iloc[index]        genanswer =  generate_answer(sample_question)        finalist.append(max_similarity_score)        finalist.append(genanswer)        #print(finalist)        return finalistdef find_index(dataset_questions, target_name):    try:        index = dataset_questions.index(target_name)        return index    except ValueError:        print(f"{target_name} not found in the list.")        return None## streamlit web appimport streamlit as stdef main():        ## giving title    st.title("Question Answer Generation")        # getting the input for user    Question1 = st.text_input("Enter Question 1")    Answer1 = st.text_input("Enter Answer1")    Question2 = st.text_input("Enter Question 2")    Answer2 = st.text_input("Enter Answer2")         # code for final generation    def run_model(Question1, Answer1, Question2, Answer2):        results=[]        input_dict = {Question1: Answer1, Question2: Answer2}        for i in input_dict:  # Adjust the loop range based on your data            index = find_index(corpus_questions, i)            correct_answers = [corpus_ans1[index], corpus_ans2[index], corpus_ans3[index], corpus_ans4[index], corpus_ans5[index], corpus_ans6[index]]            user_answer = input_dict[i]            #is_match = check_similarity(user_answer, correct_answers, index)            is_match= check_similarity(user_answer, correct_answers, index)            #adding results to dict            #add is_matchge_asnwer,i in the dict            dict1={"Question":i,"Similarity":is_match[0],"Feedback":is_match[1]}            results.append(dict1)        print("==================FINAL ANSWER====================")        return results        # creating button for prediction    generate = ''    if st.button("Generate Result"):        generate = run_model(Question1, Answer1, Question2, Answer2)    st.success(generate)    if __name__ == '__main__':    main()    